"""
ğŸ¯ ì±—ë´‡ ì„œë¹„ìŠ¤ - êµ¬í˜„ íŒŒì¼

ì´ íŒŒì¼ì€ ì±—ë´‡ì˜ í•µì‹¬ AI ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
ì•„ë˜ ì•„í‚¤í…ì²˜ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ì ‘ ì„¤ê³„í•˜ê³  êµ¬í˜„í•˜ì„¸ìš”.

ğŸ“ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ì´ˆê¸°í™” ë‹¨ê³„ (ChatbotService.__init__)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - OpenAI Client ìƒì„±                                    â”‚
â”‚  - ChromaDB ì—°ê²° (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤)                       â”‚
â”‚  - LangChain Memory ì´ˆê¸°í™” (ëŒ€í™” ê¸°ë¡ ê´€ë¦¬)               â”‚
â”‚  - Config íŒŒì¼ ë¡œë“œ                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. RAG íŒŒì´í”„ë¼ì¸ (generate_response ë‚´ë¶€)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ì‚¬ìš©ì ì§ˆë¬¸ "í•™ì‹ ì¶”ì²œí•´ì¤˜"                              â”‚
â”‚       â†“                                                  â”‚
â”‚  [_create_embedding()]                                   â”‚
â”‚       â†“                                                  â”‚
â”‚  ì§ˆë¬¸ ë²¡í„°: [0.12, -0.34, ..., 0.78]  (3072ì°¨ì›)        â”‚
â”‚       â†“                                                  â”‚
â”‚  [_search_similar()]  â† ChromaDB ê²€ìƒ‰                    â”‚
â”‚       â†“                                                  â”‚
â”‚  ê²€ìƒ‰ ê²°ê³¼: "í•™ì‹ì€ ê³¤ìê°€ê°€ ë§›ìˆì–´" (ìœ ì‚¬ë„: 0.87)        â”‚
â”‚       â†“                                                  â”‚
â”‚  [_build_prompt()]                                       â”‚
â”‚       â†“                                                  â”‚
â”‚  ìµœì¢… í”„ë¡¬í”„íŠ¸ = ì‹œìŠ¤í…œ ì„¤ì • + RAG ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LLM ì‘ë‹µ ìƒì„±                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OpenAI GPT-4 API í˜¸ì¶œ                                   â”‚
â”‚       â†“                                                  â”‚
â”‚  "í•™ì‹ì€ ê³¤ìê°€ì—ì„œ ë¨¹ëŠ” ê²Œ ì œì¼ ì¢‹ì•„! ëˆê¹ŒìŠ¤ê°€ ì¸ê¸°ì•¼"    â”‚
â”‚       â†“                                                  â”‚
â”‚  [ì„ íƒ: ì´ë¯¸ì§€ ê²€ìƒ‰]                                      â”‚
â”‚       â†“                                                  â”‚
â”‚  ì‘ë‹µ ë°˜í™˜: {reply: "...", image: "..."}                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ë©”ëª¨ë¦¬ ì €ì¥ (LangChain Memory)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ëŒ€í™” ê¸°ë¡ì— ì§ˆë¬¸-ì‘ë‹µ ì €ì¥                               â”‚
â”‚  ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ’¡ í•µì‹¬ êµ¬í˜„ ê³¼ì œ:

1. **Embedding ìƒì„±**
   - OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
   - ëª¨ë¸: text-embedding-3-large (3072ì°¨ì›)

2. **RAG ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜** â­ ê°€ì¥ ì¤‘ìš”!
   - ChromaDBì—ì„œ ìœ ì‚¬ ë²¡í„° ê²€ìƒ‰
   - ìœ ì‚¬ë„ ê³„ì‚°: similarity = 1 / (1 + distance)
   - threshold ì´ìƒì¸ ë¬¸ì„œë§Œ ì„ íƒ

3. **LLM í”„ë¡¬í”„íŠ¸ ì„¤ê³„**
   - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìºë¦­í„° ì„¤ì •)
   - RAG ì»¨í…ìŠ¤íŠ¸ í†µí•©
   - ëŒ€í™” ê¸°ë¡ í¬í•¨

4. **ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬**
   - LangChainì˜ ConversationSummaryBufferMemory ì‚¬ìš©
   - ëŒ€í™”ê°€ ê¸¸ì–´ì§€ë©´ ìë™ìœ¼ë¡œ ìš”ì•½


ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ARCHITECTURE.md: ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ëª…
- IMPLEMENTATION_GUIDE.md: ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ
- README.md: í”„ë¡œì íŠ¸ ê°œìš”


âš ï¸ ì£¼ì˜ì‚¬í•­:
- ì´ íŒŒì¼ì˜ êµ¬ì¡°ëŠ” ê°€ì´ë“œì¼ ë¿ì…ë‹ˆë‹¤
- ììœ ë¡­ê²Œ ì¬ì„¤ê³„í•˜ê³  í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë‹¨, generate_response() í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ëŠ” ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤
  (app.pyì—ì„œ í˜¸ì¶œí•˜ê¸° ë•Œë¬¸)
"""

import os
from pathlib import Path
from dotenv import load_dotenv
import json

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
BASE_DIR = Path(__file__).resolve().parent.parent


class ChatbotService:
    """
    ì±—ë´‡ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì±—ë´‡ì˜ ëª¨ë“  AI ë¡œì§ì„ ìº¡ìŠí™”í•©ë‹ˆë‹¤.
    
    ì£¼ìš” ì±…ì„:
    1. OpenAI API ê´€ë¦¬
    2. ChromaDB ë²¡í„° ê²€ìƒ‰
    3. LangChain ë©”ëª¨ë¦¬ ê´€ë¦¬
    4. ì‘ë‹µ ìƒì„± íŒŒì´í”„ë¼ì¸
    
    ì§ì ‘ êµ¬í˜„í•´ì•¼ í•  ë©”ì„œë“œ:
    - __init__: ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
    - _load_config: ì„¤ì • íŒŒì¼ ë¡œë“œ
    - _init_chromadb: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    - _create_embedding: í…ìŠ¤íŠ¸ â†’ ë²¡í„° ë³€í™˜
    - _search_similar: RAG ê²€ìƒ‰ ìˆ˜í–‰ (í•µì‹¬!)
    - _build_prompt: í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    - generate_response: ìµœì¢… ì‘ë‹µ ìƒì„± (ëª¨ë“  ë¡œì§ í†µí•©)
    """
    
    def __init__(self):
        """
        ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        TODO: ë‹¤ìŒ êµ¬ì„± ìš”ì†Œë“¤ì„ ì´ˆê¸°í™”í•˜ì„¸ìš”

        1. Config ë¡œë“œ
           - config/chatbot_config.json íŒŒì¼ ì½ê¸°
           - ì±—ë´‡ ì´ë¦„, ì„¤ëª…, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë“±

        2. OpenAI Client
           - API í‚¤: os.getenv("OPENAI_API_KEY")
           - from openai import OpenAI
           - self.client = OpenAI(api_key=...)

        3. ChromaDB
           - í…ìŠ¤íŠ¸ ì„ë² ë”© ì»¬ë ‰ì…˜ ì—°ê²°
           - ê²½ë¡œ: static/data/chatbot/chardb_embedding
           - self.collection = ...

        4. LangChain Memory (ì„ íƒ)
           - ConversationSummaryBufferMemory
           - ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
           - self.memory = ...

        íŒíŠ¸:
        - ChromaDB: import chromadb
        - LangChain: from langchain.memory import ConversationSummaryBufferMemory
        """
        print("[ChatbotService] ì´ˆê¸°í™” ì¤‘... ")

        # 1. Config ë¡œë“œ
        self.config = self._load_config()
        print(f"[ChatbotService] Config ë¡œë“œ ì™„ë£Œ: {self.config.get('name', 'Unknown')}")

        # 2. OpenAI Client ì´ˆê¸°í™”
        from openai import OpenAI
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.client = OpenAI(api_key=api_key)
        print("[ChatbotService] OpenAI Client ì´ˆê¸°í™” ì™„ë£Œ")

        # 3. ChromaDB ì´ˆê¸°í™”
        try:
            self.collection = self._init_chromadb()
            print(f"[ChatbotService] ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"[ChatbotService] ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨ (ì»¬ë ‰ì…˜ì´ ì—†ì„ ìˆ˜ ìˆìŒ): {e}")
            self.collection = None

        # 4. LangChain Memory ì´ˆê¸°í™” (ì„ íƒ)
        try:
            from langchain.memory import ConversationBufferMemory
            from langchain_openai import ChatOpenAI

            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ê°„ë‹¨í•œ ë²„í¼ ë©”ëª¨ë¦¬ ì‚¬ìš©)
            self.memory = ConversationBufferMemory(
                return_messages=True,
                memory_key="chat_history"
            )
            print("[ChatbotService] LangChain Memory ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"[ChatbotService] LangChain Memory ì´ˆê¸°í™” ì‹¤íŒ¨ (ì„ íƒ ì‚¬í•­): {e}")
            self.memory = None

        print("[ChatbotService] ì´ˆê¸°í™” ì™„ë£Œ")
    
    
    def _load_config(self):
        """
        ì„¤ì • íŒŒì¼ ë¡œë“œ

        TODO: config/chatbot_config.json ì½ì–´ì„œ ë°˜í™˜

        ë°˜í™˜ê°’ ì˜ˆì‹œ:
        {
            "name": "ê¹€ì„œê°•",
            "character": {...},
            "system_prompt": {...}
        }
        """
        config_path = BASE_DIR / "config" / "chatbot_config.json"

        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            print(f"[ERROR] ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            # ê¸°ë³¸ ì„¤ì • ë°˜í™˜
            return {
                "name": "ì±—ë´‡",
                "description": "ê¸°ë³¸ ì±—ë´‡ì…ë‹ˆë‹¤.",
                "system_prompt": {
                    "base": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                    "rules": ["ì¹œì ˆí•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”"]
                }
            }
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {
                "name": "ì±—ë´‡",
                "system_prompt": {"base": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."}
            }
    
    
    def _init_chromadb(self):
        """
        ChromaDB ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ë°˜í™˜

        TODO:
        1. PersistentClient ìƒì„±
        2. ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° (ì´ë¦„: "rag_collection")
        3. ì»¬ë ‰ì…˜ ë°˜í™˜

        íŒíŠ¸:
        - import chromadb
        - db_path = BASE_DIR / "static/data/chatbot/chardb_embedding"
        - client = chromadb.PersistentClient(path=str(db_path))
        - collection = client.get_collection(name="rag_collection")
        """
        import chromadb

        # ChromaDB ì €ì¥ ê²½ë¡œ
        db_path = BASE_DIR / "static" / "data" / "chatbot" / "chardb_embedding"

        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        db_path.mkdir(parents=True, exist_ok=True)

        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = chromadb.PersistentClient(path=str(db_path))

        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ìƒì„±)
        try:
            collection = client.get_collection(name="rag_collection")
            print(f"[ChromaDB] ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: rag_collection (ë¬¸ì„œ ìˆ˜: {collection.count()})")
        except Exception:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            collection = client.create_collection(
                name="rag_collection",
                metadata={"description": "RAGìš© í…ìŠ¤íŠ¸ ì„ë² ë”© ì»¬ë ‰ì…˜"}
            )
            print("[ChromaDB] ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: rag_collection")

        return collection
    
    
    def _create_embedding(self, text: str) -> list:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            list: 3072ì°¨ì› ë²¡í„° (text-embedding-3-large ëª¨ë¸)

        TODO:
        1. OpenAI API í˜¸ì¶œ
        2. embeddings.create() ì‚¬ìš©
        3. ë²¡í„° ë°˜í™˜

        íŒíŠ¸:
        - response = self.client.embeddings.create(
        -     input=[text],
        -     model="text-embedding-3-large"
        - )
        - return response.data[0].embedding
        """
        try:
            response = self.client.embeddings.create(
                input=[text],
                model="text-embedding-3-large"
            )
            embedding = response.data[0].embedding
            return embedding
        except Exception as e:
            print(f"[ERROR] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    
    def _search_similar(self, query: str, threshold: float = 0.45, top_k: int = 5):
        """
        RAG ê²€ìƒ‰: ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸° (í•µì‹¬ ë©”ì„œë“œ!)

        Args:
            query (str): ê²€ìƒ‰ ì§ˆì˜
            threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.3-0.5 ê¶Œì¥)
            top_k (int): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Returns:
            tuple: (document, similarity, metadata) ë˜ëŠ” (None, None, None)

        TODO: RAG ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

        1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
           query_embedding = self._create_embedding(query)

        2. ChromaDB ê²€ìƒ‰
           results = self.collection.query(
               query_embeddings=[query_embedding],
               n_results=top_k,
               include=["documents", "distances", "metadatas"]
           )

        3. ìœ ì‚¬ë„ ê³„ì‚° ë° í•„í„°ë§
           for doc, dist, meta in zip(...):
               similarity = 1 / (1 + dist)  â† ìœ ì‚¬ë„ ê³µì‹!
               if similarity >= threshold:
                   ...

        4. ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ë°˜í™˜
           return (best_document, best_similarity, metadata)


        ğŸ’¡ í•µì‹¬ ê°œë…:

        - Distance vs Similarity
          Â· ChromaDBëŠ” "ê±°ë¦¬(distance)"ë¥¼ ë°˜í™˜ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
          Â· ìš°ë¦¬ëŠ” "ìœ ì‚¬ë„(similarity)"ë¡œ ë³€í™˜ (í´ìˆ˜ë¡ ìœ ì‚¬)
          Â· ë³€í™˜ ê³µì‹: similarity = 1 / (1 + distance)

        - Threshold
          Â· 0.3: ë§¤ìš° ëŠìŠ¨í•œ ë§¤ì¹­ (ê´€ë ¨ì„± ë‚®ì•„ë„ OK)
          Â· 0.45: ì ë‹¹í•œ ë§¤ì¹­ (ì¶”ì²œ!)
          Â· 0.7: ë§¤ìš° ì—„ê²©í•œ ë§¤ì¹­ (ì •í™•í•œ ë‹µë§Œ)

        - Top K
          Â· 5-10ê°œ ì •ë„ ê²€ìƒ‰
          Â· ê·¸ ì¤‘ threshold ë„˜ëŠ” ê²ƒë§Œ ì‚¬ìš©


        ğŸ› ë””ë²„ê¹… íŒ:
        - print()ë¡œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        - ìœ ì‚¬ë„ ê°’ í™•ì¸ (ë„ˆë¬´ ë‚®ìœ¼ë©´ threshold ì¡°ì •)
        - ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© í™•ì¸
        """
        # ChromaDB ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ None ë°˜í™˜
        if self.collection is None:
            print("[RAG] ChromaDB ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return (None, None, None)

        # ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìœ¼ë©´ None ë°˜í™˜
        if self.collection.count() == 0:
            print("[RAG] ChromaDB ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            return (None, None, None)

        try:
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self._create_embedding(query)

            # 2. ChromaDB ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["documents", "distances", "metadatas"]
            )

            # 3. ìœ ì‚¬ë„ ê³„ì‚° ë° í•„í„°ë§
            documents = results['documents'][0]
            distances = results['distances'][0]
            metadatas = results['metadatas'][0] if results['metadatas'] else [{}] * len(documents)

            # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
            best_document = None
            best_similarity = 0
            best_metadata = None

            for doc, dist, meta in zip(documents, distances, metadatas):
                # ìœ ì‚¬ë„ ê³„ì‚° (ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜)
                similarity = 1 / (1 + dist)

                print(f"[RAG] ë¬¸ì„œ: {doc[:50]}... | ê±°ë¦¬: {dist:.4f} | ìœ ì‚¬ë„: {similarity:.4f}")

                # Threshold ì´ìƒì¸ ê²ƒë§Œ ì„ íƒ
                if similarity >= threshold and similarity > best_similarity:
                    best_document = doc
                    best_similarity = similarity
                    best_metadata = meta

            # 4. ê²°ê³¼ ë°˜í™˜
            if best_document:
                print(f"[RAG] âœ“ ìœ ì‚¬ ë¬¸ì„œ ë°œê²¬ (ìœ ì‚¬ë„: {best_similarity:.4f})")
                print(f"[RAG] ë¬¸ì„œ ë‚´ìš©: {best_document[:100]}...")
                return (best_document, best_similarity, best_metadata)
            else:
                print(f"[RAG] âœ— Threshold({threshold}) ì´ìƒì¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return (None, None, None)

        except Exception as e:
            print(f"[ERROR] RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return (None, None, None)
    
    
    def _build_prompt(self, user_message: str, context: str = None, username: str = "ì‚¬ìš©ì"):
        """
        LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±

        Args:
            user_message (str): ì‚¬ìš©ì ë©”ì‹œì§€
            context (str): RAG ê²€ìƒ‰ ê²°ê³¼ (ì„ íƒ)
            username (str): ì‚¬ìš©ì ì´ë¦„

        Returns:
            str: ìµœì¢… í”„ë¡¬í”„íŠ¸

        TODO:
        1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (configì—ì„œ)
        2. RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ ê²°ì •
        3. ëŒ€í™” ê¸°ë¡ í¬í•¨ (ì„ íƒ)
        4. ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ë°˜í™˜

        í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ:
        ```
        ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµ ì„ ë°° ê¹€ì„œê°•ì…ë‹ˆë‹¤.
        ì‹ ì…ìƒë“¤ì—ê²Œ í•™êµ ìƒí™œì„ ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

        [ì°¸ê³  ì •ë³´]  â† RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ
        í•™ì‹ì€ ê³¤ìê°€ê°€ ë§›ìˆì–´. ëˆê¹ŒìŠ¤ê°€ ì¸ê¸°ì•¼.

        ì‚¬ìš©ì: í•™ì‹ ì¶”ì²œí•´ì¤˜
        ```
        """
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        system_prompt = self.config.get('system_prompt', {})
        base_prompt = system_prompt.get('base', 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.')
        rules = system_prompt.get('rules', [])

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = [base_prompt]

        # ê·œì¹™ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if rules:
            prompt_parts.append("\n[ëŒ€í™” ê·œì¹™]")
            for rule in rules:
                prompt_parts.append(f"- {rule}")

        # 2. RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨
        if context:
            prompt_parts.append(f"\n[ì°¸ê³  ì •ë³´]\n{context}")

        # 3. ëŒ€í™” ê¸°ë¡ í¬í•¨ (ì„ íƒ)
        if self.memory:
            try:
                chat_history = self.memory.load_memory_variables({})
                if chat_history and 'chat_history' in chat_history:
                    history = chat_history['chat_history']
                    if history:
                        prompt_parts.append("\n[ìµœê·¼ ëŒ€í™”]")
                        # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
                        recent_messages = history[-6:] if len(history) > 6 else history
                        for msg in recent_messages:
                            role = "ì‚¬ìš©ì" if msg.type == "human" else "ì±—ë´‡"
                            prompt_parts.append(f"{role}: {msg.content}")
            except Exception as e:
                print(f"[WARN] ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 4. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        prompt_parts.append(f"\n{username}: {user_message}")

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ë°˜í™˜
        final_prompt = "\n".join(prompt_parts)
        return final_prompt
    
    
    def generate_response(self, user_message: str, username: str = "ì‚¬ìš©ì") -> dict:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì±—ë´‡ ì‘ë‹µ ìƒì„±
        
        Args:
            user_message (str): ì‚¬ìš©ì ì…ë ¥
            username (str): ì‚¬ìš©ì ì´ë¦„
        
        Returns:
            dict: {
                'reply': str,       # ì±—ë´‡ ì‘ë‹µ í…ìŠ¤íŠ¸
                'image': str|None   # ì´ë¯¸ì§€ ê²½ë¡œ (ì„ íƒ)
            }
        
        
        TODO: ì „ì²´ ì‘ë‹µ ìƒì„± íŒŒì´í”„ë¼ì¸ êµ¬í˜„
        
        
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ğŸ“‹ êµ¬í˜„ ë‹¨ê³„
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        [1ë‹¨ê³„] ì´ˆê¸° ë©”ì‹œì§€ ì²˜ë¦¬
        
            if user_message.strip().lower() == "init":
                # ì²« ì¸ì‚¬ë§ ë°˜í™˜
                bot_name = self.config.get('name', 'ì±—ë´‡')
                return {
                    'reply': f"ì•ˆë…•! ë‚˜ëŠ” {bot_name}ì´ì•¼.",
                    'image': None
                }
        
        
        [2ë‹¨ê³„] RAG ê²€ìƒ‰ ìˆ˜í–‰
        
            context, similarity, metadata = self._search_similar(
                query=user_message,
                threshold=0.45,
                top_k=5
            )
            
            has_context = (context is not None)
        
        
        [3ë‹¨ê³„] í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        
            prompt = self._build_prompt(
                user_message=user_message,
                context=context,
                username=username
            )
        
        
        [4ë‹¨ê³„] LLM API í˜¸ì¶œ
        
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # ë˜ëŠ” gpt-4
                messages=[
                    {"role": "system", "content": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )
            
            reply = response.choices[0].message.content
        
        
        [5ë‹¨ê³„] ë©”ëª¨ë¦¬ ì €ì¥ (ì„ íƒ)
        
            if self.memory:
                self.memory.save_context(
                    {"input": user_message},
                    {"output": reply}
                )
        
        
        [6ë‹¨ê³„] ì‘ë‹µ ë°˜í™˜
        
            return {
                'reply': reply,
                'image': None  # ì´ë¯¸ì§€ ê²€ìƒ‰ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            }
        
        
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        1. RAG í™œìš©
           - ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
           - ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
        
        2. ì—ëŸ¬ ì²˜ë¦¬
           - try-exceptë¡œ API ì˜¤ë¥˜ ì²˜ë¦¬
           - ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
        
        3. ë¡œê¹…
           - ê° ë‹¨ê³„ë§ˆë‹¤ print()ë¡œ ìƒíƒœ ì¶œë ¥
           - ë””ë²„ê¹…ì— ë§¤ìš° ìœ ìš©!
        
        4. í™•ì¥ì„±
           - ì´ë¯¸ì§€ ê²€ìƒ‰ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
           - ê°ì • ë¶„ì„ ì¶”ê°€ ê°€ëŠ¥
           - ë‹¤ì¤‘ ì–¸ì–´ ì§€ì› ê°€ëŠ¥
        
        
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ğŸ› ë””ë²„ê¹… ì˜ˆì‹œ
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        print(f"\n{'='*50}")
        print(f"[USER] {username}: {user_message}")
        print(f"[RAG] Context found: {has_context}")
        if has_context:
            print(f"[RAG] Similarity: {similarity:.4f}")
            print(f"[RAG] Context: {context[:100]}...")
        print(f"[LLM] Calling API...")
        print(f"[BOT] {reply}")
        print(f"{'='*50}\n")
        """

        print(f"\n{'='*50}")
        print(f"[USER] {username}: {user_message}")

        try:
            # [1ë‹¨ê³„] ì´ˆê¸° ë©”ì‹œì§€ ì²˜ë¦¬
            if user_message.strip().lower() == "init":
                bot_name = self.config.get('name', 'ì±—ë´‡')
                greeting = f"ì•ˆë…•! ë‚˜ëŠ” {bot_name}ì´ì•¼. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë´!"
                print(f"[BOT] (ì´ˆê¸° ì¸ì‚¬) {greeting}")
                print(f"{'='*50}\n")
                return {
                    'reply': greeting,
                    'image': None
                }

            # [2ë‹¨ê³„] RAG ê²€ìƒ‰ ìˆ˜í–‰
            context, similarity, metadata = self._search_similar(
                query=user_message,
                threshold=0.45,
                top_k=5
            )

            has_context = (context is not None)

            # [3ë‹¨ê³„] í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_prompt(
                user_message=user_message,
                context=context,
                username=username
            )

            # ë””ë²„ê¹… ì¶œë ¥
            if has_context:
                print(f"[RAG] âœ“ Context found (ìœ ì‚¬ë„: {similarity:.4f})")
                print(f"[RAG] Context preview: {context[:100]}...")
            else:
                print(f"[RAG] âœ— No context found (ì¼ë°˜ ëŒ€í™” ëª¨ë“œ)")

            # [4ë‹¨ê³„] LLM API í˜¸ì¶œ
            print(f"[LLM] Calling OpenAI API...")

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
            system_prompt_config = self.config.get('system_prompt', {})
            system_message = system_prompt_config.get('base', 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.')

            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )

            reply = response.choices[0].message.content

            print(f"[LLM] âœ“ Response generated")
            print(f"[BOT] {reply[:100]}...")

            # [5ë‹¨ê³„] ë©”ëª¨ë¦¬ ì €ì¥ (ì„ íƒ)
            if self.memory:
                try:
                    self.memory.save_context(
                        {"input": user_message},
                        {"output": reply}
                    )
                    print(f"[MEMORY] âœ“ Conversation saved")
                except Exception as e:
                    print(f"[WARN] ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

            # [6ë‹¨ê³„] ì‘ë‹µ ë°˜í™˜
            print(f"{'='*50}\n")
            return {
                'reply': reply,
                'image': None  # ì´ë¯¸ì§€ ê²€ìƒ‰ ë¡œì§ì€ ì¶”í›„ ì¶”ê°€ ê°€ëŠ¥
            }

        except Exception as e:
            print(f"[ERROR] ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            print(f"{'='*50}\n")
            return {
                'reply': "ì£„ì†¡í•´ìš”, ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                'image': None
            }


# ============================================================================
# ì‹±ê¸€í†¤ íŒ¨í„´
# ============================================================================
# ChatbotService ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì•± ì „ì²´ì—ì„œ ì¬ì‚¬ìš©
# (ë§¤ë²ˆ ìƒˆë¡œ ì´ˆê¸°í™”í•˜ë©´ ë¹„íš¨ìœ¨ì )

_chatbot_service = None

def get_chatbot_service():
    """
    ì±—ë´‡ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)
    
    ì²« í˜¸ì¶œ ì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±, ì´í›„ ì¬ì‚¬ìš©
    """
    global _chatbot_service
    if _chatbot_service is None:
        _chatbot_service = ChatbotService()
    return _chatbot_service


# ============================================================================
# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
# ============================================================================

if __name__ == "__main__":
    """
    ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
    
    ì‹¤í–‰ ë°©ë²•:
    python services/chatbot_service.py
    """
    print("ì±—ë´‡ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    service = get_chatbot_service()
    
    # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    response = service.generate_response("init", "í…ŒìŠ¤í„°")
    print(f"ì´ˆê¸° ì‘ë‹µ: {response}")
    
    # ì¼ë°˜ ëŒ€í™” í…ŒìŠ¤íŠ¸
    response = service.generate_response("ì•ˆë…•í•˜ì„¸ìš”!", "í…ŒìŠ¤í„°")
    print(f"ì‘ë‹µ: {response}")
